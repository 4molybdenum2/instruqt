challenges:
- assignment: '

    Deploying the bridge on OpenShift is really easy using the new `KafkaBridge` custom
    resource provided by the Red Hat AMQ Streams Cluster Operator.


    ### Logging in to the Cluster using the OpenShift CLI


    Before creating any applications, log in as admin. This is required if you want
    to log in to the web console and use it.


    To log in to the OpenShift cluster from the _Terminal_ run:


    ```

    oc login -u developer -p developer

    ```


    This will log you in using the credentials:


    * **Username:** ``developer``

    * **Password:** ``developer``


    Use the same credentials to log in to the web console.


    ### Switch your own namespace


    Switch to the (project) namespace called ``kafka`` where the Cluster Operator
    manages the Kafka resources:


    ```

    oc project kafka

    ```


    ### Deploying Kafka Bridge to your OpenShift cluster


    The deployment uses a YAML file to provide the specification to create a `KafkaBridge`
    resource.


    Click the link below to open the custom resource (CR) definition for the bridge:


    * `kafka-bridge.yaml`{{open}}


    The bridge has to connect to the Apache Kafka cluster. This is specified in the
    `bootstrapServers` property. The bridge then uses a native Apache Kafka consumer
    and producer for interacting with the cluster.


    >For information about configuring the KafkaBridge resource, see [Kafka Bridge
    configuration](https://access.redhat.com/documentation/en-us/red_hat_amq/2020.q4/html-single/using_amq_streams_on_openshift/index#assembly-config-kafka-bridge-str).


    Deploy the Kafka Bridge with the custom image:


    ``oc -n kafka apply -f /root/projects/http-bridge/kafka-bridge.yaml``{{execute
    interrupt}}


    The Kafka Bridge node should be deployed after a few moments. To watch the pods
    status run the following command:


    ```

    oc get pods -w -l app.kubernetes.io/name=kafka-bridge

    ```


    You will see the pods changing the status to `running`. It should look similar
    to the following:


    ```bash

    NAME                                READY   STATUS              RESTARTS   AGE

    my-bridge-bridge-6b6d9f785c-dp6nk   0/1     ContainerCreating   0          5s

    my-bridge-bridge-6b6d9f785c-dp6nk   0/1     ContainerCreating   0          12s

    my-bridge-bridge-6b6d9f785c-dp6nk   0/1     Running             0          27s

    my-bridge-bridge-6b6d9f785c-dp6nk   1/1     Running             0          45s

    ```


    > This step might take a couple minutes.


    Press <kbd>Ctrl</kbd>+<kbd>C</kbd> to stop the process.


    `^C`{{execute ctrl-seq}}


    ### Creating an OpenShift route


    After deployment, the Kafka Bridge can only be accessed by applications running
    in the same OpenShift cluster. If you want to make the Kafka Bridge accessible
    to applications running outside of the OpenShift cluster, you can expose it manually
    by using one of the following features:


    * Services of types LoadBalancer or NodePort

    * Kubernetes Ingress

    * OpenShift Routes


    An OpenShift `route` is an OpenShift resource for allowing external access through
    HTTP/HTTPS to internal services such as the Kafka bridge. We will use this approach
    for our example.


    Run the following command to expose the bridge service as an OpenShift route:


    ``oc expose svc my-bridge-bridge-service``{{execute interrupt}}


    When the route is created, the Kafka Bridge is reachable through the `https://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com`
    host. You can now use any HTTP client to interact with the REST API exposed by
    the bridge to send and receive messages without needing to use the native Kafka
    protocol.

    '
  difficulty: intermediate
  slug: step1
  tabs:
  - hostname: crc-nonest-1
    title: cli
    type: terminal
  - hostname: crc-nonest-1
    port: 30443
    title: web-ui
    type: service
  timelimit: 300
  title: Deploying the HTTP Bridge
  type: challenge
- assignment: "To verify that the Ingress is working properly, try to access the ``/healthy`\
    \ endpoint of the bridge with the following curl command:\n\n``curl -ik https://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/healthy``{{execute\
    \ interrupt}}\n\nIf the bridge is reachable through the OpenShift route, it will\
    \ return an HTTP response with `200 OK` HTTP code, but an empty body.\n\nIt should\
    \ look similar to the following example of the output:\n\n```sh\nHTTP/2 200\n\
    server: nginx/1.15.0\ndate: Tue, 01 Dec 2020 15:01:22 GMT\ncontent-length: 0\n\
    cache-control: private\nset-cookie: 93b1d08256cbf837e3463c0bba903028=0e558f788ca3bde0c6204c8d9bc783e0;\
    \ Path=/; HttpOnly; Secure; SameSite=None\nvia: 1.1 google\nalt-svc: clear\n```\n\
    \n### Producing messages\n\nThe Kafka cluster we are working with has topic auto-creation\
    \ enabled, so we can immediately start to send messages through the `/topics/my-topic`\
    \ endpoint exposed by the HTTP bridge.\n\nThe bridge exposes two main REST endpoints\
    \ in order to send messages:\n\n* /topics/{topicname}\n* /topics/{topicname}/partitions/{partitionid}\n\
    \nThe first endpoint sends a message to a topic `topicname`. The second endpoint\
    \ allows the user to specify the partition using a `partitionid`. Actually, even\
    \ using the first endpoint we can specify the destination partition in the body\
    \ of the message.\n\nThe HTTP request payload is always in JSON format, but the\
    \ message values can be JSON or binary (encoded in base64 because we are sending\
    \ binary data in a JSON payload so encoding in a string format is needed).\n\n\
    When performing producer operations, `POST` requests must provide `Content-Type`\
    \ headers specifying the desired _embedded data format_, either as `json` or `binary`.\
    \ In this scenario we will be using the **JSON** format.\n\nLet's send a couple\
    \ of messages to the `my-topic` topic:\n\n```\ncurl -s -k -X POST https://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/topics/my-topic\
    \ -H 'content-type: application/vnd.kafka.json.v2+json' -d '{ \"records\": [ {\"\
    key\": \"key-1\",\"value\": \"sales-lead-0001\"}, {\"key\": \"key-2\",\"value\"\
    : \"sales-lead-0002\"} ] }' | jq\n```\n\nIf the request is successful, the Kafka\
    \ Bridge returns a `200 OK` HTTP code, with a JSON payload describing the `offsets`\
    \ array, and the partition and offsets in which the messages are written.\n\n\
    In this case, the auto-created topic has just one partition, so the response will\
    \ look something like this:\n\n```json\n{\n   \"offsets\":[\n      {\n       \
    \  \"partition\":0,\n         \"offset\":0\n      },\n      {\n         \"partition\"\
    :0,\n         \"offset\":1\n      }\n   ]\n}\n```\n\nExcellent! You have sent\
    \ your first messages to your Kafka topic through the Kafka Bridge. Now you are\
    \ ready to start consuming those messages.\n"
  difficulty: intermediate
  slug: step2
  tabs:
  - hostname: crc-nonest-1
    title: cli
    type: terminal
  - hostname: crc-nonest-1
    port: 30443
    title: web-ui
    type: service
  timelimit: 300
  title: Producing messages
  type: challenge
- assignment: "Before you can perform any consumer operations in the Kafka cluster,\
    \ you must first create a consumer by using the consumers endpoint.\n\nYou create\
    \ a consumer through the `/consumers/{groupid}` endpoint by sending an HTTP POST\
    \ with a body containing some of the supported configuration parameters, the name\
    \ of the consumer and the data format.\n\n### Creating a consumer\n\nWe are going\
    \ to create a Kafka Bridge consumer named `my-consumer` that will join the consumer\
    \ group `my-group`.\n\nExecute the following command to create the consumer:\n\
    \n```\ncurl -s -k -X POST https://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/consumers/my-group\
    \ -H 'content-type: application/vnd.kafka.v2+json' -d '{\"name\": \"my-consumer\"\
    ,\"format\": \"json\",\"auto.offset.reset\": \"earliest\",\"fetch.min.bytes\"\
    : 512,\"enable.auto.commit\": false}' | jq\n```\n\nThis will create a Kafka consumer\
    \ connected to the Kafka cluster. If the request is successful, the bridge will\
    \ reply back with a `200 OK` HTTP code, and a JSON payload with the consumer ID\
    \ (`instance_id`) and base URL (`base_uri`).\n\nIt should look similar to the\
    \ following example of the output:\n\n```json\n{\n   \"instance_id\":\"my-consumer\"\
    ,\n   \"base_uri\":\"http://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/consumers/my-group/instances/my-consumer\"\
    \n}\n```\n\n### Subscribing to a topic\n\nThe most common way for a Kafka consumer\
    \ to get messages from a topic is to subscribe to that topic as part of a consumer\
    \ group and have partitions assigned automatically.\n\nUsing the HTTP bridge,\
    \ subscription is possible through an HTTP POST to the `/consumers/{groupid}/instances/{name}/subscription`\
    \ endpoint, providing a list of topics to subscribe to or a topic pattern in a\
    \ JSON formatted payload.\n\nSubscribe your `my-consumer` consumer to the `my-topic`\
    \ topic with the following command:\n\n```\ncurl -i -k -X POST https://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/consumers/my-group/instances/my-consumer/subscription\
    \ -H 'content-type: application/vnd.kafka.v2+json' -d '{\"topics\": [\"my-topic\"\
    ]}'\n```\n\n>This will return a `204 OK` HTTP code with an empty body.\n\nThe\
    \ `topics` array can contain a single topic (as shown here) or multiple topics.\
    \ If you want to subscribe the consumer to multiple topics that match a regular\
    \ expression, you can use the `topic_pattern` string instead of the `topics` array.\n\
    \nAfter subscribing a Kafka Bridge consumer to topics, you can retrieve messages\
    \ from the consumer.\n"
  difficulty: intermediate
  slug: step3
  tabs:
  - hostname: crc-nonest-1
    title: cli
    type: terminal
  - hostname: crc-nonest-1
    port: 30443
    title: web-ui
    type: service
  timelimit: 300
  title: Subscribing to the topic
  type: challenge
- assignment: "Finally, retrieve the latest messages from the Kafka Bridge consumer\
    \ by requesting data from the `/consumers/{groupid}/instances/{name}/records`\
    \ endpoint.\n\n### Retrieving the latest messages from a Kafka Bridge consumer\n\
    \nUsing a HTTP GET method against the records endpoint performs a _poll_ for retrieving\
    \ messages from the already subscribed topics. The first poll operation after\
    \ the subscription doesn\u2019t always return records. because it just starts\
    \ the join operation of the consumer to the group, and the rebalancing in order\
    \ to get partitions assigned. Doing the next poll returns the messages if there\
    \ are any in the topic.\n\nSubmit a `GET` request to the `records` endpoint:\n\
    \n```\ncurl -s -k -X GET https://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/consumers/my-group/instances/my-consumer/records\
    \ -H 'accept: application/vnd.kafka.json.v2+json' | jq\n```\n\nRepeat step a couple\
    \ more times until you retrieve all messages from the Kafka Bridge consumer.\n\
    \n```\ncurl -s -k -X GET https://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/consumers/my-group/instances/my-consumer/records\
    \ -H 'accept: application/vnd.kafka.json.v2+json' | jq\n```\n\nThe Kafka Bridge\
    \ returns an array of messages\u2009\u2014\u2009describing the topic name, key,\
    \ value, partition, and offset\u2009\u2014\u2009in the response body, along with\
    \ a `200` code. Messages are retrieved from the latest offset by default.\n\n\
    You should get an output similar to the following:\n\n```js\n[  {\n    \"topic\"\
    : \"my-topic\",\n    \"key\": \"key-1\",\n    \"value\": \"sales-lead-0001\",\n\
    \    \"partition\": 0,\n    \"offset\": 0\n  },\n  {\n    \"topic\": \"my-topic\"\
    ,\n    \"key\": \"key-2\",\n    \"value\": \"sales-lead-0002\",\n    \"partition\"\
    : 0,\n    \"offset\": 1\n  }\n]\n```\n\n### Committing offsets to the log\n\n\
    Next, use the `/consumers/{groupid}/instances/{name}/offsets` endpoint to manually\
    \ commit offsets to the log for all messages received by the Kafka Bridge consumer.\
    \ This is required because the Kafka Bridge consumer that you created earlier\
    \ was configured with the `enable.auto.commit` setting as false.\n\nCommit offsets\
    \ to the log for `my-consumer`:\n\n```\ncurl -i -k -X POST https://my-bridge-bridge-service-kafka.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/consumers/my-group/instances/my-consumer/offsets\n\
    ```\n\n>Because no request body is submitted, offsets are committed for all the\
    \ records that have been received by the consumer. Alternatively, the request\
    \ body can contain an array (OffsetCommitSeekList) that specifies the topics and\
    \ partitions that you want to commit offsets for.\n\nIf the request is successful,\
    \ the Kafka Bridge returns a `204` code only.\n\nCongratulations! You were able\
    \ to deploy the AMQ Streams Kafka Bridge and connect to a Kafka cluster using\
    \ HTTP. You sent some message to an example topic and then created a consumer\
    \ to retrieve messages. Finally you committed the offsets the messages.\n"
  difficulty: intermediate
  slug: step4
  tabs:
  - hostname: crc-nonest-1
    title: cli
    type: terminal
  - hostname: crc-nonest-1
    port: 30443
    title: web-ui
    type: service
  timelimit: 300
  title: Consuming messages
  type: challenge
developers:
- btannous@redhat.com
- nvinto@redhat.com
- rjarvine@redhat.com
icon: https://logodix.com/logo/1910931.png
level: beginner
notes:
- contents: 'Apache Kafka uses a custom protocol on top of TCP/IP for communication
    between applications and the Kafka cluster. Clients are supported in many different
    programming languages, but there are certain scenarios where it is not possible
    to use such clients. In this situation, you can use the standard HTTP/1.1 protocol
    to access Kafka instead.


    The Red Hat AMQ Streams Kafka Bridge provides an API for integrating HTTP-based
    clients with a Kafka cluster running on AMQ Streams. Applications can perform
    typical operations such as:


    * Sending messages to topics

    * Subscribing to one or more topics

    * Receiving messages from the subscribed topics

    * Committing offsets related to the received messages

    * Seeking to a specific position


    As with AMQ Streams, the Kafka Bridge is deployed into an OpenShift cluster using
    the AMQ Streams Cluster Operator, or installed on Red Hat Enterprise Linux using
    downloaded files.


    ![HTTP integration](https://access.redhat.com/webassets/avalon/d/Red_Hat_AMQ-7.7-Using_AMQ_Streams_on_OpenShift-en-US/images/750556a6bc4af4daeca4b1df0fd24835/kafka-bridge.png)


    In the following tutorial, you will deploy the Kafka Bridge and use it to connect
    to your Apache Kafka cluster using HTTP.

    '
  type: text
owner: openshift
private: false
published: true
skipping_enabled: false
slug: kafka-http-bridge
tags:
- openshift
title: Exposing Apache Kafka through the HTTP Bridge
type: track
