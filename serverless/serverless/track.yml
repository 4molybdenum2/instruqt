challenges:
- assignment: "[serverless-install-script]: https://github.com/openshift-labs/learn-katacoda/blob/master/developing-on-openshift/serverless/assets/01-prepare/install-serverless.bash\n\
    [olm-docs]: https://docs.openshift.com/container-platform/latest/operators/understanding/olm/olm-understanding-olm.html\n\
    [serving-docs]: https://github.com/knative/serving-operator#the-knativeserving-custom-resource\n\
    \nOpenShift Serverless is an OpenShift add-on that can be installed via an operator\
    \ that is available within the OpenShift OperatorHub.\n\nSome operators are able\
    \ to be installed into single namespaces within a cluster and are only able to\
    \ monitor resources within that namespace.  The OpenShift Serverless operator\
    \ is one that installs globally on a cluster so that it is able to monitor and\
    \ manage Serverless resources for every single project and user within the cluster.\n\
    \nYou could install the Serverless operator using the *Operators* tab within the\
    \ web console, or you can use the CLI tool `oc`.  In this instance, the terminal\
    \ on the side is already running through an automated CLI install.  This [script\
    \ can be found here][serverless-install-script].\n\nSince the install will take\
    \ some time, let's take a moment to review the installation via the web console.\n\
    \n> **Note:** *These steps are for informational purposes only. **Do not** follow\
    \ them in this instance as there already is an automated install running in the\
    \ terminal.*\n\n## Log in and install the operator\nThis section is automated,\
    \ so you won't need to install the operator.  If you wanted to reproduce these\
    \ results on another cluster, you'd need to authenticate as an admin to complete\
    \ the following steps:\n\n![01-login](/openshift/assets/developing-on-openshift/serverless/01-prepare/01-login.png)\n\
    \nCluster administrators can install the `OpenShift Serverless` operator via *Operator\
    \ Hub*\n\n![02-operatorhub](/openshift/assets/developing-on-openshift/serverless/01-prepare/02-operatorhub.png)\n\
    \n> **Note:** *We can inspect the details of the `serverless-operator` packagemanifest\
    \ within the CLI via `oc describe packagemanifest serverless-operator -n openshift-marketplace`.*\n\
    >\n> **Tip:** *You can find more information on how to add operators on the [OpenShift\
    \ OLM Documentation Page][olm-docs].*\n\nNext, our scripts will install the Serverless\
    \ Operator into the `openshift-operators` project using the `stable` update channel.\n\
    \n![03-serverlessoperator](/openshift/assets/developing-on-openshift/serverless/01-prepare/03-serverlessoperator.png)\n\
    \nOpen the **Installed Operators** tab and watch the **OpenShift Serverless Operator**.\
    \  The operator is installed and ready when the `Status=Succeeded`.\n\n![05-succeeded](/openshift/assets/developing-on-openshift/serverless/01-prepare/05-succeeded.png)\n\
    \n> **Note:** *We can inspect the additional api resouces that the serverless\
    \ operator added to the cluster via the CLI command `oc api-resources | egrep\
    \ 'Knative|KIND'`*.\n\nNext, we need to use these new resources provided by the\
    \ serverless operator to install KnativeServing.\n\n## Install KnativeServing\n\
    As per the [Knative Serving Operator documentation][serving-docs] you must create\
    \ a `KnativeServing` object to install Knative Serving using the OpenShift Serverless\
    \ Operator.\n\n> **Note:** *Remember, these steps are for informational purposes\
    \ only. **Do not** follow them in this instance as there already is an automated\
    \ install running in the terminal.*\n\nFirst we create the `knative-serving` project.\n\
    \n![06-kservingproject](/openshift/assets/developing-on-openshift/serverless/01-prepare/06-kservingproject.png)\n\
    \nWithin the `knative-serving` project open the **Installed Operators** tab and\
    \ the **OpenShift Serverless Operator**.  Then create an instance of **Knative\
    \ Serving**.\n\n![07-kservinginstance](/openshift/assets/developing-on-openshift/serverless/01-prepare/07-kservinginstance.png)\n\
    \n![08-kservinginstance](/openshift/assets/developing-on-openshift/serverless/01-prepare/08-kservinginstance.png)\n\
    \nOpen the Knative Serving instance.  It is deployed when the **Condition** `Ready=True`.\n\
    \n![09-kservingready](/openshift/assets/developing-on-openshift/serverless/01-prepare/09-kservingready.png)\n\
    \nOpenShift Serverless should now be installed!\n\n## Login as a Developer and\
    \ Create a Project\nBefore beginning we should change to the non-privileged user\
    \ `developer` and create a new `project` for the tutorial.\n\n> **Note:** *Remember,\
    \ these steps are for informational purposes only. **Do not** follow them in this\
    \ instance as there already is an automated install running in the terminal.*\n\
    \nTo change to the non-privileged user in our environment we login as username:\
    \ `developer`, password: `developer`\n\nNext create a new project by executing:\
    \ `oc new-project serverless-tutorial`\n\nThere we go! You are all set to kickstart\
    \ your serverless journey with **OpenShift Serverless**. \n\nPlease check for\
    \ the terminal output of `Serverless Tutorial Ready!` before continuing.  Once\
    \ ready, click `continue` to go to the next module on how to deploy your first\
    \ severless service.\n"
  difficulty: intermediate
  slug: 01-prepare
  tabs:
  - hostname: crc-nonest-1
    title: CLI
    type: terminal
  - hostname: crc-nonest-1
    port: 30443
    title: OpenShift Web Console
    type: service
  - hostname: crc-nonest-1
    path: /root
    title: Visual Editor
    type: code
  timelimit: 300
  title: Prepare for Exercises
  type: challenge
- assignment: "[ocp-serving-components]: https://docs.openshift.com/container-platform/4.7/serverless/architecture/serverless-serving-architecture.html\n\
    \nAt the end of this chapter you will be able to:\n- Deploy your very first application\
    \ as an OpenShift Serverless `Service`.\n- Learn the underlying components of\
    \ a Serverless Service, such as: `configurations`, `revisions`, and `routes`.\n\
    - Watch the service `scale to zero`.\n- `Delete` the Serverless Service.\n\nNow\
    \ that we have OpenShift Serverless installed on the cluster, we can deploy our\
    \ first Serverless application, creating a Knative service. But before doing that,\
    \ let's explore the Serving module.\n\n## Explore Serving\nLet's take a moment\
    \ to explore the new API resources available in the cluster since installing `Serving`.\n\
    \nLike before, we can see what ```\napi-resources` are available now by running:\
    \ `oc api-resources --api-group serving.knative.dev\n```\n\n> **Note:** *Before\
    \ we searched for any `api-resource` which had `KnativeServing` in any of the\
    \ output using `grep`.  In this section we are filtering the `APIGROUP` which\
    \ equals `serving.knative.dev`.*\n\nThe Serving module consists of a few different\
    \ pieces.  These pieces are listed in the output of the previous command: `configurations`,\
    \ `revisions`, `routes`, and `services`.  The `knativeservings` api-resource was\
    \ existing, and we already created an instance of it to install KnativeServing.\
    \ \n\n```bash\nNAME              SHORTNAMES      APIGROUP              NAMESPACED\
    \   KIND\nconfigurations    config,cfg      serving.knative.dev   true       \
    \  Configuration\nknativeservings   ks              serving.knative.dev   true\
    \         KnativeServing\nrevisions         rev             serving.knative.dev\
    \   true         Revision\nroutes            rt              serving.knative.dev\
    \   true         Route\nservices          kservice,ksvc   serving.knative.dev\
    \   true         Service\n```\n\nThe diagram below shows how each of the components\
    \ of the Serving module fit together.\n\n![Serving](/openshift/assets/developing-on-openshift/serverless/02-serving/serving.png)\n\
    \nWe will discuss what each one of these new resources are used for in the coming\
    \ sections.  Let's start with `services`.\n\n## OpenShift Serverless Services\n\
    As discussed in the [OpenShift Serverless Documentation][ocp-serving-components],\
    \ a **Knative Service Resource** automatically manages the whole lifecycle of\
    \ a serverless workload on a cluster. It controls the creation of other objects\
    \ to ensure that an app has a route, a configuration, and a new revision for each\
    \ update of the service. Services can be defined to always route traffic to the\
    \ latest revision or to a pinned revision.\n\nBefore deploying your first Serverless\
    \ Service, let us take a moment to understand it's structure:\n\n```yaml\n# ./assets/02-serving/service.yaml\n\
    \napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: greeter\n\
    \  namespace: serverless-tutorial\nspec:\n  template:\n    spec:\n      containers:\n\
    \      - image: quay.io/rhdevelopers/knative-tutorial-greeter:quarkus\n      \
    \  livenessProbe:\n          httpGet:\n            path: /healthz\n        readinessProbe:\n\
    \          httpGet:\n            path: /healthz\n\n```\n\nWe now are deploying\
    \ an instance of a `Service` that is provided by `serving.knative.dev`.  In our\
    \ simple example we define a container `image` and the paths for `health checking`\
    \ of the service.  We also provided the `name` and `namespace`.\n\n## Deploy the\
    \ Serverless Service\nTo deploy the service we could deploy the yaml above by\
    \ executing `oc apply -n serverless-tutorial -f 02-serving/service.yaml`, but\
    \ one of the best features of serverless is the ability to deploy and work with\
    \ serverless resources without ever working with yaml.  In this tutorial we will\
    \ use the `kn` CLI tool to work with serverless. \n\nTo deploy the service execute:\n\
    ```bash\nkn service create greeter \\\n   --image quay.io/rhdevelopers/knative-tutorial-greeter:quarkus\
    \ \\\n   --namespace serverless-tutorial\n```\n`\n```\n\n> **Note:** *The equivalent\
    \ yaml for the service above can be seen by executing: ```\ncat 02-serving/service.yaml\n\
    ```*.\n\nWatch the status using the commands:\n```bash\n# ./assets/02-serving/watch-service.bash\n\
    \n#!/usr/bin/env bash\nwhile : ;\ndo\n  output=`oc get pod -n serverless-tutorial`\n\
    \  echo \"$output\"\n  if [ -z \"${output##*'Running'*}\" ] ; then echo \"Service\
    \ is ready.\"; break; fi;\n  sleep 5\ndone\n\n```\n`\n```\n\nA successful service\
    \ deployment will show the following `greeter` pods:\n\n```shell\nNAME       \
    \                                 READY   STATUS    RESTARTS   AGE\ngreeter-6vzt6-deployment-5dc8bd556c-42lqh\
    \   2/2     Running   0          11s\n```\n\n> **Question:** *If you run the watch\
    \ script too late you might not see any pods running or being created after a\
    \ few loops and will have to escape out of the watch with `CTRL+C`.  I'll let\
    \ you think about why this happens.  Continue on for now and validate the deployment.*\n\
    \n## Check out the deployment\nAs discussed in the [OpenShift Serverless Documentation][ocp-serving-components],\
    \ Serverless Service deployments will create a few required serverless resources.\
    \  We will dive into each of them below.\n\n### Service\nWe can see the Serverless\
    \ Service that we just created by executing: ```\nkn service list\n```\n\nThe\
    \ output should be similar to:\n\n```bash\nNAME      URL                     \
    \                                LATEST            AGE     CONDITIONS   READY\
    \   REASON\ngreeter   http://greeter-serverless-tutorial.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com\
    \  greeter-fyrxn-1   6m28s   3 OK / 3     True    \n```\n\n> **Note:** *It also\
    \ is possible to use the ```\noc` command to see the serverless resources, to\
    \ see the services execute: `oc get -n serverless-tutorial services.serving.knative.dev\n\
    ```*\n\nThe Serverless `Service` gives us information about it's `URL`, it's `LATEST`\
    \ revision, it's `AGE`, and it's status for each service we have deployed.  It\
    \ is also important to see that `READY=True` to validate that the service has\
    \ deployed successfully even if there were no pods running in the previous section.\n\
    \nIt also is possible to ```\ndescribe` a specific service to gather detailed\
    \ information about that service by executing: `kn service describe greeter\n\
    ```\n\nThe output should be similar to:\n```bash\nName:       greeter\nNamespace:\
    \  serverless-tutorial\nAge:        15m\nURL:        http://greeter-serverless-tutorial.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com\n\
    \nRevisions:  \n  100%  @latest (greeter-fyrxn-1) [1] (15m)\n        Image:  quay.io/rhdevelopers/knative-tutorial-greeter:quarkus\
    \ (pinned to 767e2f)\n\nConditions:  \n  OK TYPE                   AGE REASON\n\
    \  ++ Ready                  14m \n  ++ ConfigurationsReady    14m \n  ++ RoutesReady\
    \            14m \n```\n\n> **Note:** *Most resources can be `described` via the\
    \ `kn` tool.  Be sure to check them out while continuing along the tutorial.*\n\
    \nNext, we will inspect the `Route`.  Routes manage the ingress and URL into the\
    \ service.\n\n> *How is it possible to have a service deployed and `Ready` but\
    \ no pods are running for that service?*\n>\n> See a hint by inspecting the ```\n\
    READY` column from `oc get deployment\n```\n\n### Route\nAs the [OpenShift Serverless\
    \ Documentation][ocp-serving-components] explains, a `Route` resource maps a network\
    \ endpoint to one or more Knative revisions. It is possible to manage the traffic\
    \ in several ways, including fractional traffic and named routes.  Currently,\
    \ since our service is new, we have only one revision to direct our users to --\
    \ in later sections we will show how to manage multiple revisions at once using\
    \ a `Canary Deployment Pattern`.\n\nWe can see the route by executing: ```\nkn\
    \ route list\n```\n\nSee the `NAME` of the route, the `URL`, as well as if it\
    \ is `READY`:\n```bash\nNAME      URL                                        \
    \             READY\ngreeter   http://greeter-serverless-tutorial.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com\
    \  True\n```\n\n### Revision\nLastly, we can inspect the `Revisions`.  As per\
    \ the [OpenShift Serverless Documentation][ocp-serving-components], a `Revision`\
    \ is a point-in-time snapshot of the code and configuration for each modification\
    \ made to the workload. Revisions are immutable objects and can be retained for\
    \ as long as needed. Cluster administrators can modify the `revision.serving.knative.dev`\
    \ resource to enable automatic scaling of Pods in an OpenShift Container Platform\
    \ cluster.\n\nBefore inspecting revisions, update the image of the service by\
    \ executing:\n```bash\nkn service update greeter \\\n   --image quay.io/rhdevelopers/knative-tutorial-greeter:latest\
    \ \\\n   --namespace serverless-tutorial\n```\n`\n```.\n\n> **Note:** *Updating\
    \ the image of the service will create a new revision, or point-in-time snapshot\
    \ of the workload.*\n \nWe can see the revision by executing: ```\nkn revision\
    \ list\n```\n\nThe output should be similar to:\n```bash\nNAME              SERVICE\
    \   TRAFFIC   TAGS   GENERATION   AGE     CONDITIONS   READY   REASON\ngreeter-qxcrc-2\
    \   greeter   100%             2            6m35s   3 OK / 4     True    \ngreeter-fyrxn-1\
    \   greeter                    1            33m     3 OK / 4     True   \n```\n\
    \nHere we can see each revision and details including the percantage of `TRAFFIC`\
    \ it is receiving.  We can also see the generational number of this revision,\
    \ **which is incremented on each update of the service**.\n\n### Invoke the Service\n\
    Now that we have seen a a few of the underlying resouces that get created when\
    \ deploying a Serverless Service, we can test the deployment.  To do so we will\
    \ need to use the URL returned by the serverless route.  To invoke the service\
    \ we can execute the command ```\ncurl http://greeter-serverless-tutorial.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com\n\
    ```\n\nThe service will return a response like **Hi  greeter => '6fee83923a9f'\
    \ : 1**\n\n> **NOTE:** *You can also open this in your own local browser to test\
    \ the service!*\n\n### Scale to Zero\nThe ```\ngreeter` service will automatically\
    \ scale down to zero if it does not get request for approximately 90 seconds.\
    \  Try watching the service scaling down by executing `oc get pods -n serverless-tutorial\
    \ -w\n```\n\nTry invoking the service again using the `curl` from earlier to see\
    \ the service scaling back up from zero.\n\n> **Question:** *Do you see now why\
    \ the pod might not have been running earlier? The service scaled to zero before\
    \ you checked!*\n\n## Delete the Service\nWe can easily delete our service by\
    \ executing: ```\nkn service delete greeter\n```\n\nAwesome! You have successfully\
    \ deployed your very first serverless service using OpenShift Serverless. In the\
    \ next chapter we will go a bit deeper in understanding how to distribute traffic\
    \ between multiple revisions of the same service.\n"
  difficulty: intermediate
  slug: 02-serving
  tabs:
  - hostname: crc-nonest-1
    title: CLI
    type: terminal
  - hostname: crc-nonest-1
    port: 30443
    title: OpenShift Web Console
    type: service
  - hostname: crc-nonest-1
    path: /root
    title: Visual Editor
    type: code
  timelimit: 300
  title: Deploying your service
  type: challenge
- assignment: "At the end of this step you will be able to:\n- Provide a custom `name`\
    \ to the deployment\n- Configure a service to use a `blue-green` deployment pattern\n\
    \nServerless services always route traffic to the **latest** revision of the deployment.\
    \ In this section we will learn a few different ways to split the traffic between\
    \ revisions in our service.\n\n## Revision Names\nBy default, Serverless generates\
    \ random revision names for the service that is based on using the Serverless\
    \ service\u2019s `metadata.name` as a prefix.\n\nThe following service deployment\
    \ uses the same greeter service as the last section in the tutorial except it\
    \ is configured with an arbitrary revision name.\n\nLet's deploy the greeter service\
    \ again, but this time set its **revision name** to `greeter-v1` by executing:\n\
    ```bash\nkn service create greeter \\\n   --image quay.io/rhdevelopers/knative-tutorial-greeter:quarkus\
    \ \\\n   --namespace serverless-tutorial \\\n   --revision-name greeter-v1\n```\n\
    `\n``` \n\n> **Note:** *The equivalent yaml for the service above can be seen\
    \ by executing: ```\ncat 03-traffic-distribution/greeter-v1-service.yaml\n```*.\n\
    \nNext, we are going to update the greeter service to add a message prefix environment\
    \ variable and change the revision name to `greeter-v2`.  Do so by executing:\n\
    ```bash\nkn service update greeter \\\n   --image quay.io/rhdevelopers/knative-tutorial-greeter:quarkus\
    \ \\\n   --namespace serverless-tutorial \\\n   --revision-name greeter-v2 \\\n\
    \   --env MESSAGE_PREFIX=GreeterV2\n```\n`\n``` \n\n> **Note:** *The equivalent\
    \ yaml for the service above can be seen by executing: ```\ncat 03-traffic-distribution/greeter-v2-service.yaml\n\
    ```*.\n\nSee that the two greeter services have deployed successfully by executing\
    \ ```\nkn revision list\n```\n\nThe last command should output two revisions,\
    \ `greeter-v1` and `greeter-v2`:\n\n```bash\nNAME         SERVICE   TRAFFIC  \
    \ TAGS   GENERATION   AGE    CONDITIONS   READY   REASON\ngreeter-v2   greeter\
    \   100%             2            4m5s   3 OK / 4     True    \ngreeter-v1   greeter\
    \                    1            14m    3 OK / 4     True \n```\n\n> **Note:**\
    \ *It is important to notice that by default the latest revision will replace\
    \ the previous by receiving 100% of the traffic.*\n\n## Blue-Green Deployment\
    \ Patterns\nServerless offers a simple way of switching 100% of the traffic from\
    \ one Serverless service revision (blue) to another newly rolled out revision\
    \ (green).  We can rollback to a previous revision if any new revision (e.g. green)\
    \ has any unexpected behaviors.\n\nWith the deployment of `greeter-v2` serverless\
    \ automatically started to direct 100% of the traffic to `greeter-v2`. Now let\
    \ us assume that we need to roll back `greeter-v2` to `greeter-v1` for some reason.\n\
    \nUpdate the greeter service by executing:\n```bash\nkn service update greeter\
    \ \\\n   --traffic greeter-v1=100 \\\n   --tag greeter-v1=current \\\n   --tag\
    \ greeter-v2=prev \\\n   --tag @latest=latest\n```\n`\n```\n\nThe above service\
    \ definition creates three sub-routes(named after traffic tags) to the existing\
    \ `greeter` route.\n- **current**: The revision will receive 100% of the traffic\
    \ distribution\n- **prev**: The previously active revision, which will now receive\
    \ no traffic\n- **latest**: The route pointing to the latest service deployment,\
    \ here we change the default configuration to receive no traffic.\n\n> **Note:**\
    \ *Be sure to notice the special tag: `latest` in our configuration above.  In\
    \ the configuration we defined 100% of the traffic be handled by `greeter-v1`.*\n\
    >\n> *Using the latest tag allows changing the default behavior of services to\
    \ route 100% of the traffic to the latest revision.*\n\nWe can validate the service\
    \ traffic tags by executing: ```\nkn route describe greeter\n```\n\nThe output\
    \ should be similar to:\n\n```bash\nName:       greeter\nNamespace:  serverless-tutorial\n\
    Age:        1m\nURL:        http://greeter-serverless-tutorial.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com\n\
    Service:    greeter\n\nTraffic Targets:  \n    0%  @latest (greeter-v2) #latest\n\
    \        URL:  http://latest-greeter-serverless-tutorial.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com\n\
    \  100%  greeter-v1 #current\n        URL:  http://current-greeter-serverless-tutorial.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com\n\
    \    0%  greeter-v2 #prev\n        URL:  http://prev-greeter-serverless-tutorial.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com\n\
    \nConditions:  \n  OK TYPE                  AGE REASON\n  ++ Ready           \
    \      23s \n  ++ AllTrafficAssigned     1m \n  ++ IngressReady          23s \n\
    ```\n\n> **Note:** *The equivalent yaml for the service above can be seen by executing:\
    \ ```\ncat 03-traffic-distribution/service-pinned.yaml\n```*.\n\nThe configuration\
    \ change we issued above does not create any new `configuration`, `revision`,\
    \ or `deployment` as there was no application update (e.g. image tag, env var,\
    \ etc).  When calling the service without the sub-route, Serverless scales up\
    \ the `greeter-v1`, as our configuration specifies and the service responds with\
    \ the text `Hi greeter \u21D2 '9861675f8845' : 1`.\n\nWe can check that ```\n\
    greeter-v1` is receiving 100% of the traffic now to our main route by executing:\
    \ `curl http://greeter-serverless-tutorial.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com\n\
    ```\n\n> **Challenge:** *As a test, route all of the traffic back to `greeter-v2`\
    \ (green).*\n\nCongrats! You now are able to apply a a `blue-green` deployment\
    \ pattern using Serverless.  In the next section we will look at `canary release`\
    \ deployments.\n"
  difficulty: intermediate
  slug: 03-traffic-distribution
  tabs:
  - hostname: crc-nonest-1
    title: CLI
    type: terminal
  - hostname: crc-nonest-1
    port: 30443
    title: OpenShift Web Console
    type: service
  - hostname: crc-nonest-1
    path: /root
    title: Visual Editor
    type: code
  timelimit: 300
  title: Traffic Distribution
  type: challenge
- assignment: "At the end of this step you will be able to:\n- Configure a service\
    \ to use a `Canary Release` deployment pattern\n\n> **Note:** *If you did not\
    \ complete the previous Traffic Distribution section please execute both of the\
    \ following commands:*\n\n```bash\nkn service create greeter --image quay.io/rhdevelopers/knative-tutorial-greeter:quarkus\
    \ --namespace serverless-tutorial --revision-name greeter-v1\nkn service update\
    \ greeter --image quay.io/rhdevelopers/knative-tutorial-greeter:quarkus --namespace\
    \ serverless-tutorial --revision-name greeter-v2 --env MESSAGE_PREFIX=GreeterV2\n\
    ```\n`\n```\n\n## Applying a Canary Release Pattern\nA Canary release is more\
    \ effective when looking to reduce the risk of introducing new features. Using\
    \ this type of deployment model allows a more effective feature-feedback loop\
    \ before rolling out the change to the entire user base.  Using this deployment\
    \ approach with Serverless allows splitting the traffic between revisions in increments\
    \ as small as 1%.\n\nTo see this in action, apply the following service update\
    \ that will split the traffic 80% to 20% between `greeter-v1` and `greeter-v2`\
    \ by executing:\n```bash\nkn service update greeter \\\n   --traffic greeter-v1=80\
    \ \\\n   --traffic greeter-v2=20 \\\n   --traffic @latest=0\n```\n`\n```\n\nIn\
    \ the service configuration above see the 80/20 split between v1 and v2 of the\
    \ greeter service.  Also see that the current service is set to receive 0% of\
    \ the traffic using the `latest` tag.\n\n> **Note:** *The equivalent yaml for\
    \ the service above can be seen by executing: ```\ncat 04-canary-releases/greeter-canary-service.yaml\n\
    ```\n\nAs in the previous section on Applying Blue-Green Deployment Pattern deployments,\
    \ the command will not create a new configuration, revision, or deployment.\n\n\
    To observe the new traffic distribution execute the following:\n\n```bash\n# ./assets/04-canary-releases/poll-svc-10.bash\n\
    \n#!/usr/bin/env bash\nfor run in {1..10}\ndo\n  curl http://greeter-serverless-tutorial.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com\n\
    done\n```\n`\n```\n\n80% of the responses are returned from greeter-v1 and 20%\
    \ from greeter-v2. See the listing below for sample output:\n\n```bash\nHi  greeter\
    \ => '6fee83923a9f' : 1\nHi  greeter => '6fee83923a9f' : 2\nHi  greeter => '6fee83923a9f'\
    \ : 3\nGreeterV2  greeter => '4d1c551aac4f' : 1\nHi  greeter => '6fee83923a9f'\
    \ : 4\nHi  greeter => '6fee83923a9f' : 5\nHi  greeter => '6fee83923a9f' : 6\n\
    GreeterV2  greeter => '4d1c551aac4f' : 2\nHi  greeter => '6fee83923a9f' : 7\n\
    Hi  greeter => '6fee83923a9f' : 8\n```\n\nAlso notice that two pods are running,\
    \ representing both greeter-v1 and greeter-v2: ```\noc get pods -n serverless-tutorial\n\
    ```\n\n```bash\nNAME                                     READY   STATUS    RESTARTS\
    \   AGE\ngreeter-v1-deployment-5dc8bd556c-42lqh   2/2     Running   0        \
    \  29s\ngreeter-v2-deployment-1dc2dd145c-41aab   2/2     Running   0         \
    \ 20s\n```\n\n> **Note:** *If we waited too long to execute the preceding command\
    \ we might have noticed the services scaling to zero!*\n>\n> **Challenge:** *As\
    \ a challenge, adjust the traffic distribution percentages and observe the responses\
    \ by executing the `poll-svc-10.bash` script again.*\n\n## Delete the Service\n\
    \nWe will need to cleanup the project for our next section by executing: ```\n\
    kn service delete greeter\n```\n\nCongrats! You now are able to apply a few different\
    \ deployment patterns using Serverless.  In the next section we will see how we\
    \ dig a little deeper into the scaling components of Serverless.\n"
  difficulty: intermediate
  slug: 04-canary-releases
  tabs:
  - hostname: crc-nonest-1
    title: CLI
    type: terminal
  - hostname: crc-nonest-1
    port: 30443
    title: OpenShift Web Console
    type: service
  - hostname: crc-nonest-1
    path: /root
    title: Visual Editor
    type: code
  timelimit: 300
  title: Canary Releases
  type: challenge
- assignment: "[apachebench]: https://httpd.apache.org/docs/2.4/programs/ab.html \n\
    [learn-katacoda]: https://github.com/openshift-labs/learn-katacoda\n\nAt the end\
    \ of this chapter you will be able to:\n- Understand `scale-to-zero` in depth\
    \ and why it\u2019s important.\n- Understand how to configure the `scale-to-zero-grace-period`.\n\
    - Understand types of `autoscaling strategies`.\n- Enable `concurrency based autoscaling`.\n\
    - Configure a `minimum number of replicas` for a service.\n- Configure a `Horizontal\
    \ Pod Autoscaler` for a service.\n\n## In depth: Scaling to Zero\nAs you might\
    \ recall from the `Deploying your Service` section of the tutorial, Scale-to-zero\
    \ is one of the main properties of Serverless. After a defined time of idleness\
    \ *(called the `stable-window`)* a revision is considered inactive, which causes\
    \ a few things to happen.  First off, all routes pointing to the now inactive\
    \ revision will be pointed to the so-called **activator**. \n\n![serving-flow](/openshift/assets/developing-on-openshift/serverless/05-scaling/serving-flow.png)\n\
    \nThe name `activator` is somewhat misleading these days.  Originally it used\
    \ to activate inactive revisions, hence the name.  Today its primary responsibilites\
    \ are to receive and buffer requests for revisions that are inactive as well as\
    \ report metrics to the autoscaler.  \n\nAfter the revision has been deemed idle,\
    \ by not receiving any traffic during the `stable-window`, the revision will be\
    \ marked inactive.  If **scaling to zero** is enabled then there is an additional\
    \ grace period before the inactive revision terminates, called the `scale-to-zero-grace-period`.\
    \  When **scaling to zero** is enabled the total termination period is equal to\
    \ the sum of both the `stable-window` (default=60s) and `scale-to-zero-grace-period`\
    \ (default=30s) = default=90s.\n\nIf we try to access the service while it is\
    \ scaled to zero the activator will pick up the request(s) and buffer them until\
    \ the **Autoscaler** is able to quickly create pods for the given revision.\n\n\
    > **Note:** *You might have noticed an initial lag when trying to access your\
    \ service.  The reason for that delay is highly likely that your request is being\
    \ held by the activator!*\n\nFirst login as an administrator for the cluster:\
    \ ```\noc login -u admin -p admin\n```\n\nIt is possible to see the default configurations\
    \ of the autoscaler by executing: ```\noc -n knative-serving describe cm config-autoscaler\n\
    ```\n\nHere we can see the `stable-window`, `scale-to-zero-grace-period`, a `enable-scale-to-zero`,\
    \ amongst other settings.\n\n```bash\n...\n# When operating in a stable mode,\
    \ the autoscaler operates on the\n# average concurrency over the stable window.\n\
    # Stable window must be in whole seconds.\nstable-window: \"60s\"\n...\n# Scale\
    \ to zero feature flag\nenable-scale-to-zero: \"true\"\n...\n# Scale to zero grace\
    \ period is the time an inactive revision is left\n# running before it is scaled\
    \ to zero (min: 30s).\nscale-to-zero-grace-period: \"30s\"\n...\n```\n\nIn this\
    \ tutorial leave the configuration as-is, but if there were reasons to change\
    \ them it is possible to edit this configmap as needed.\n\n> **Tip:** Another,\
    \ possibly better, way to make those changes would be to add configuration to\
    \ the `KnativeServing` instance that was applied in the `Prepare for Exercises`\
    \ section early in this tutorial.\n>\n> Open and inspect that yaml by executing:\
    \ ```\ncat 01-prepare/serving.yaml\n```\n>\n> There are other settings of Serverless\
    \ available.  It is possible to describe other configmaps in the `knative-serving`\
    \ project to find them.\n>\n> Explore what all is available by running: ```\n\
    oc get cm -n knative-serving\n```\n\nNow, log back in as the developer as we do\
    \ not need elevated privileges to continue: ```\noc login -u developer -p developer\n\
    ```\n\n## Minimum Scale\nBy default, Serverless Serving allows for 100 concurrent\
    \ requests into each revision and allows the service to scale down to zero.  This\
    \ property optimizes the application as it does not use any resources for running\
    \ idle processes!  This is the out of the box configuration, and it works quite\
    \ well depending on the needs of the specific application.\n\nSometimes application\
    \ traffic is unpredictable, bursting often, and when the app is scaled to zero\
    \ it takes some time to come back up -- giving a slow start to the first users\
    \ of the app.\n\nTo solve for this, services are able to be configured to allow\
    \ a few processes to sit idle, waiting for the initial users.  This is configured\
    \ by specifying a minimum scale for the service via an the annotation `autoscaling.knative.dev/minScale`.\n\
    \n> **Note:** *You can also limit your maximum pods using `autoscaling.knative.dev/maxScale`*\n\
    \n```yaml\n# ./assets/05-scaling/service-min-max-scale.yaml\n\napiVersion: serving.knative.dev/v1\n\
    kind: Service\nmetadata:\n  name: prime-generator\n  namespace: serverless-tutorial\n\
    spec:\n  template:\n    metadata:\n      annotations:\n        # the minimum number\
    \ of pods to scale down to\n        autoscaling.knative.dev/minScale: \"2\"\n\
    \        # the maximum number of pods to scale up to\n        autoscaling.knative.dev/maxScale:\
    \ \"5\"\n    spec:\n      containers:\n        - image: quay.io/rhdevelopers/prime-generator:v27-quarkus\n\
    \          livenessProbe:\n            httpGet:\n              path: /healthz\n\
    \          readinessProbe:\n            httpGet:\n              path: /healthz\n\
    \n```\n\nIn the definition above the minimum scale is configured to 2 and the\
    \ maximum scale to 5 via two annotations.\n\nSince serverless allows deploying\
    \ without yaml we will continue to use the `kn` command instead of the above yaml\
    \ service definition.\n\nDeploy the service by executing:\n```bash\nkn service\
    \ create prime-generator \\\n   --namespace serverless-tutorial \\\n   --annotation\
    \ autoscaling.knative.dev/minScale=2 \\\n   --annotation autoscaling.knative.dev/maxScale=5\
    \ \\\n   --image quay.io/rhdevelopers/prime-generator:v27-quarkus\n```\n`\n```\n\
    \nSee that the ```\nprime-generator` is deployed and it will never be scaled outside\
    \ of 2-5 pods available by checking: `oc get pods -n serverless-tutorial\n```\n\
    \nThis now guarantee that there will always be at least two instances available\
    \ at all times to provide the service with no initial lag at the cost of consuming\
    \ additional resources.  Next, test the service won't scale past 5.\n\nTo load\
    \ the service we will use [apachebench (ab)][apachebench].  We will configure\
    \ `ab` to send 2550 total requests `-n 2550`, of which 850 will be performed concurrently\
    \ each time `-c 850`.  Immediatly after we will show the deployments in the project\
    \ to be able to see the number of pods running.\n\n```\nab -n 2550 -c 850 -t 60\
    \ \"http://prime-generator-serverless-tutorial.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/?sleep=3&upto=10000&memload=100\"\
    \ && oc get deployment -n serverless-tutorial\n```\n\n> **Note:** *This might\
    \ take a few moments!*\n\nNotice that `5/5` pods should be marked as `READY`,\
    \ confirming the max scale.\n\n## AutoScaling\nAs mentioned before, Serverless\
    \ by default will scale up when there are 100 concurrent requests coming in at\
    \ one time.  This scaling factor might work well for some applications, but not\
    \ all -- fortunately this is a tuneable factor!  In some cases you might notice\
    \ that a given app is not using its resources too effectively as each request\
    \ is CPU-bound.\n\nTo help with this, it is possible to adjust the service to\
    \ scale up sooner, say 50 concurrent requests via configuring an annotation of\
    \ `autoscaling.knative.dev/target`.\n\nUpdate the prime-generator service by executing:\n\
    ```bash\nkn service update prime-generator \\\n   --annotation autoscaling.knative.dev/target=50\n\
    ```\n`\n```\n\n> **Note:** *The equivalent yaml for the service above can be seen\
    \ by executing: ```\ncat 05-scaling/service-50.yaml\n```*.\n\nAgain test the scaling\
    \ by loading the service.  This time send 275 concurrent requests totaling 1100.\n\
    \n```\nab -n 1100 -c 275 -t 60 \"http://prime-generator-serverless-tutorial.[[HOST_SUBDOMAIN]]-80-[[KATACODA_HOST]].environments.katacoda.com/?sleep=3&upto=10000&memload=100\"\
    \ && oc get deployment -n serverless-tutorial\n```\n\nNotice that at least 6 pods\
    \ should be up and running.  There might be more than 6 as `ab` could be overloading\
    \ the amount of concurrent workers at one time.\n\nThis will work well, but given\
    \ that this application is CPU-bound instead of request bound we might want to\
    \ choose a different autoscaling class that is based on CPU load to be able to\
    \ manage scaling more effectively.\n\n## HPA AutoScaling\nCPU based autoscaling\
    \ metrics are achieved using something called a Horizontal Pod Autoscaler (HPA).\
    \  In this example we want to scale up when the service starts using 70% of the\
    \ CPU.  Do this by adding three new annotations to the service: `autoscaling.knative.dev/{metric,target,class}`\n\
    \nUpdate the prime-generator service by executing:\n```bash\nkn service update\
    \ prime-generator \\\n   --annotation autoscaling.knative.dev/minScale- \\\n \
    \  --annotation autoscaling.knative.dev/maxScale- \\\n   --annotation autoscaling.knative.dev/target=70\
    \ \\\n   --annotation autoscaling.knative.dev/metric=cpu \\\n   --annotation autoscaling.knative.dev/class=hpa.autoscaling.knative.dev\n\
    ```\n`\n```\n\n> **Note:** *Notice that the above `kn` command removes, adds,\
    \ and updates existing annotations to the service.  To delete use `\u2014annotation\
    \ name-`.*\n>\n> *Getting the service to scale on the large CPU nodes that this\
    \ tutorial is running on is relatively hard.  If you have any ideas to see this\
    \ in action put an issue in at [this tutorial's github][learn-katacoda]*\n>\n\
    > *The equivalent yaml for the service above can be seen by executing: ```\ncat\
    \ 05-scaling/service-hpa.yaml\n```*.\n\n\n## Delete the Service\n\nCleanup the\
    \ project using: ```\nkn service delete prime-generator\n```\n\nCongrats! You\
    \ are now a Serverless Scaling Expert!  We can now adjust and tune Serverless\
    \ scaling using concurrency or CPU based HPAs.\n"
  difficulty: intermediate
  slug: 05-scaling
  tabs:
  - hostname: crc-nonest-1
    title: CLI
    type: terminal
  - hostname: crc-nonest-1
    port: 30443
    title: OpenShift Web Console
    type: service
  - hostname: crc-nonest-1
    path: /root
    title: Visual Editor
    type: code
  timelimit: 300
  title: Scaling
  type: challenge
description: '[serverless-main]: https://www.openshift.com/learn/topics/serverless

  [amq-docs]: https://developers.redhat.com/products/amq/overview

  [pipelines-main]: https://www.openshift.com/learn/topics/pipelines

  [service-mesh-main]: https://www.openshift.com/learn/topics/service-mesh


  In this self-paced tutorial, you will learn the basics of how to use OpenShift Serverless,
  which provides a development model to remove the overhead of server provisioning
  and maintenance from the developer.


  In this tutorial, you will:

  * Deploy an OpenShift Serverless `service`.

  * Deploy multiple `revisions` of a service.

  * Understand the `underlying compents` of a serverless service.

  * Understand how Serverless is able to `scale-to-zero`.

  * Run different revisions of a service via `canary` and `blue-green` deployments.

  * Utilize the `knative client`.


  ## Why Serverless?


  Deploying applications as Serverless services is becoming a popular architectural
  style. It seems like many organizations assume that _Functions as a Service (FaaS)_
  implies a serverless architecture. We think it is more accurate to say that FaaS
  is one of the ways to utilize serverless, although it is not the only way. This
  raises a super critical question for enterprises that may have applications which
  could be monolith or a microservice: What is the easiest path to serverless application
  deployment?


  The answer is a platform that can run serverless workloads, while also enabling
  you to have complete control of the configuration, building, and deployment. Ideally,
  the platform also supports deploying the applications as linux containers.


  ## OpenShift Serverless


  In this chapter we introduce you to one such platform -- [OpenShift Serverless][serverless-main].  OpenShift
  Serverless helps developers to deploy and run applications that will scale up or
  scale to zero on-demand. Applications are packaged as OCI compliant Linux containers
  that can be run anywhere.  This is known as `Serving`.


  ![OpenShift Serving](/openshift/assets/developing-on-openshift/serverless/00-intro/knative-serving-diagram.png)


  Serverless has a robust way to allow for applications to be triggered by a variety
  of event sources, such as events from your own applications, cloud services from
  multiple providers, Software as a Service (SaaS) systems and Red Hat Services ([AMQ
  Streams][amq-docs]).  This is known as `Eventing`.


  ![OpenShift Eventing](/openshift/assets/developing-on-openshift/serverless/00-intro/knative-eventing-diagram.png)


  OpenShift Serverless applications can be integrated with other OpenShift services,
  such as OpenShift [Pipelines][pipelines-main], and [Service Mesh][service-mesh-main],
  delivering a complete serverless application development and deployment experience.


  This tutorial will focus on the `Serving` aspect of OpenShift Serverless as the
  first diagram showcases.  Be on the lookout for additional tutorials to dig further
  into Serverless, specifically `Eventing`.


  ## The Environment


  During this scenario, you will be using a hosted OpenShift environment that is created
  just for you. This environment is not shared with other users of the system. Because
  each user completing this scenario has their own environment, we had to make some
  concessions to ensure the overall platform is stable and used only for this training.
  For that reason, your environment will only be active for a one hour period. Keep
  this in mind before you get started on the content. Each time you start this training,
  a new environment will be created on the fly.


  The OpenShift environment created for you is running version 4.7 of the OpenShift
  Container Platform. This deployment is a self-contained environment that provides
  everything you need to be successful learning the platform. This includes a preconfigured
  command line environment, the OpenShift web console, public URLs, and sample applications.


  > **Note:** *It is possible to skip around in this tutorial.  The only pre-requisite
  for each section would be the initial `Prepare for Exercises` section.*

  >

  > *For example, you could run the `Prepare for Exercises` section immediately followed
  by the `Scaling` section.*


  Now, let''s get started!

  '
developers:
- btannous@redhat.com
- nvinto@redhat.com
- rjarvine@redhat.com
icon: https://logodix.com/logo/1910931.png
level: beginner
owner: openshift
private: true
published: false
skipping_enabled: false
slug: serverless-serverless
tags:
- openshift
title: Getting Started with OpenShift Serverless
type: track
