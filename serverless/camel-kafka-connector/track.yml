challenges:
- assignment: 'In order to run this tutorial, you will need access to an OpenShift
    environment.

    And we also have access to Kafka cluster in the environment. Let''s setup the
    fundamentals.


    This tutorial is an simple *Managed File Transfer* scenario, where a laboratory
    uploads medical reports to an online object store, and we will need to transfer
    the file from the cloud object store to our local FTP server in order for the
    legacy system to consume.


    ![overview](/openshift/assets/middleware/middleware-camelk/camel-kafka-connector/camel-kafka-step01-overview.png)


    ## Logging in to the Cluster via CLI


    Before creating any applications, login as admin. This will be required if you
    want to log in to the web console and

    use it.


    To login to the OpenShift cluster from the _Terminal_ run:


    ```

    oc login -u admin -p admin

    ```


    This will log you in using the credentials:


    * **Username:** ``admin``

    * **Password:** ``admin``


    Use the same credentials to log into the web console.


    (OPTIONAL): Click the [Console](https://console-openshift-console-[[HOST_SUBDOMAIN]]-443-[[KATACODA_HOST]].environments.katacoda.com)
    tab to open the dashboard.


    ## Create a new namespace for this tutorial


    To create a new project called ``camel-kafka`` run the command:


    ```

    oc project camel-kafka

    ```


    ## Start up the Kafka Cluster and the Kafka Connect


    AMQ Streams simplifies the process of running Apache Kafka in an Openshift cluster.

    Now let''s deploy a Kafka broker cluster, the AMQ Streams Operator is already
    subscribed in the cluster,

    simply create the cluster by defining the resource definition:


    ```

    oc create -f strimzi/kafka-cluster.yaml

    ```


    Below indicates AMQ Streams has accept the configuration, and now starting to
    process.

    ```

    kafka.kafka.strimzi.io/my-cluster created

    ```


    If everything goes right, you will be able to see the pods initiated in the namespace:

    ```

    oc get pod -w

    ```


    ```

    amq-streams-cluster-operator-v1.5.3-7bbf5cdfdc-4kq7q   1/1     Running   0          3m15s

    my-cluster-entity-operator-59cf586599-vdmk5            0/3     Running   0          7s

    my-cluster-kafka-0                                     2/2     Running   0          40s

    my-cluster-kafka-1                                     2/2     Running   0          40s

    my-cluster-kafka-2                                     2/2     Running   0          40s

    my-cluster-zookeeper-0                                 1/1     Running   0          71s

    my-cluster-zookeeper-1                                 1/1     Running   0          71s

    my-cluster-zookeeper-2                                 1/1     Running   0          71s

    ```

    Ctrl+C to exit the mode.

    '
  difficulty: basic
  slug: step1
  tabs:
  - hostname: crc-nonest-1
    title: CLI
    type: terminal
  - hostname: crc-nonest-1
    port: 30443
    title: OpenShift Web Console
    type: service
  - hostname: crc-nonest-1
    path: /root
    title: Visual Editor
    type: code
  timelimit: 300
  title: Step 1
  type: challenge
- assignment: "## Setup the Minio generic object datastore\n\nLets start Minio, it\
    \ provide a S3 compatible protocol for storing the objects.\nTo create the minio\
    \ backend, just apply the provided file:\n\n```\noc apply -f minio/minio.yaml\n\
    ```\n\nAnd now, you have a working generic object datastore.\n\n\n## Setup the\
    \ SFTP Server for the file store\n\nRun the following script to start up the SFTP\
    \ Server in the system:\n\n```\nbash sftp/sftp.sh\n```\n\nIf everything goes as\
    \ planned, you should see the following message:\n```\nclusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid\
    \ added: \"default\"\n--> Found container image d87b9ff (10 hours old) from Docker\
    \ Hub for \"atmoz/sftp\"\n\n    * An image stream tag will be created as \"ftpserver:latest\"\
    \ that will track this image\n    * This image will be deployed in deployment\
    \ config \"ftpserver\"\n    * Port 22/tcp will be load balanced by service \"\
    ftpserver\"\n      * Other containers can access this service through the hostname\
    \ \"ftpserver\"\n    * WARNING: Image \"atmoz/sftp\" runs as the 'root' user which\
    \ may not be permitted by your cluster administrator\n\n--> Creating resources\
    \ ...\n    imagestream.image.openshift.io \"ftpserver\" created\n    deploymentconfig.apps.openshift.io\
    \ \"ftpserver\" created\n    service \"ftpserver\" created\n--> Success\n    Application\
    \ is not exposed. You can expose services to the outside world by executing one\
    \ or more of the commands below:\n     'oc expose service/ftpserver'\n    Run\
    \ 'oc status' to view your app.\nsecret/ftp-users created\nsecret/sftp-ssh-key\
    \ created\ninfo: Generated volume name: volume-dpt6b\ndeploymentconfig.apps.openshift.io/ftpserver\
    \ volume updated\ninfo: Generated volume name: volume-mrld4\ndeploymentconfig.apps.openshift.io/ftpserver\
    \ volume updated\n```\n\nTake note at the secret/sftp-ssh-key, we will be using\
    \ this secret for configuring our sink connector.\n\n```\noc get secret/sftp-ssh-key\n\
    ```\n\nWaiting until the _RUNNING_ status\n\n```\noc get pod -w | grep ftpserver\n\
    ```\n\nHit Ctrl+C to exit the mode.\n\nTake a look at the folder, and it should\
    \ be empty at the moment. This is where we want our file files to be at.\n\n```\n\
    oc exec -i `oc get pod -l deploymentconfig=ftpserver -o=jsonpath='{.items[0].metadata.name}'`\
    \ -- ls /home/foo/upload/\n```\n\n\n## Setup Source2Image\n\nFor the Kafka Connect\
    \ image, we will need the connector libraries to run.\nSource2Image will be used\
    \ to setup and build the connectors.Go to the text editor on the right, under\
    \ the folder /root/camel-kafka. Right click on the directory and choose New ->\
    \ File and name it `kafka-connect-s2i.yaml`.\n\nPaste the following code into\
    \ the application.\n\n<pre class=\"file\" data-filename=\"kafka-connect-s2i.yaml\"\
    \ data-target=\"replace\">\napiVersion: kafka.strimzi.io/v1beta1\nkind: KafkaConnectS2I\n\
    metadata:\n name: my-connect-cluster\nspec:\n  bootstrapServers: 'my-cluster-kafka-bootstrap:9093'\n\
    \  config:\n    config.storage.replication.factor: 1\n    config.storage.topic:\
    \ connect-cluster-configs\n    group.id: connect-cluster\n    offset.storage.replication.factor:\
    \ 1\n    offset.storage.topic: connect-cluster-offsets\n    status.storage.replication.factor:\
    \ 1\n    status.storage.topic: connect-cluster-status\n  externalConfiguration:\n\
    \    volumes:\n      - name: sftp-ssh-key\n        secret:\n          secretName:\
    \ sftp-ssh-key\n  replicas: 1\n  tls:\n    trustedCertificates:\n      - certificate:\
    \ ca.crt\n        secretName: my-cluster-cluster-ca-cert\n  version: 2.5.0\n</pre>\n\
    \nNotice here it connects to the Kafka cluster that we initiated in the previous\
    \ step, and also points to the secret in the SFTP server (we will need this later)\
    \ as an external configuration. Go ahead execute the command to setup the KafkaConnect\
    \ Source2Image.\n\n```\noc apply -f camel-kafka/kafka-connect-s2i.yaml\n```\n\n\
    \nEnable Kafka Connectors to be able to instantiated via custom resource:\n```\n\
    oc annotate kafkaconnects2is my-connect-cluster strimzi.io/use-connector-resources=true\n\
    ```\n"
  difficulty: basic
  slug: step2
  tabs:
  - hostname: crc-nonest-1
    title: CLI
    type: terminal
  - hostname: crc-nonest-1
    port: 30443
    title: OpenShift Web Console
    type: service
  - hostname: crc-nonest-1
    path: /root
    title: Visual Editor
    type: code
  timelimit: 300
  title: Step 2
  type: challenge
- assignment: "\n## Add and build the Camel Kafka Connector binary to the connector\n\
    \nTake a look at the plugin folder, you will find two folders named\n\n```\nls\
    \ ./plugin/\n```\n\n- camel-minio-kafka-connector\n- camel-sftp-kafka-connector\n\
    \nThey are the libraries needed for the connectors. Now let's use Source2Image\
    \ to add these connectors libraries to the existing images. We now need to build\
    \ the connectors and add them to the image.\n\n```\noc start-build my-connect-cluster-connect\
    \ --from-dir=./plugin/ --follow\n```\n\n\nWait for the rollout of the new image\
    \ to finish and the replica set with the new connector to become ready. Once it\
    \ is done, we can check that the connectors are available in our Kafka Connect\
    \ cluster. Since AMQ Streams is running Kafka Connect in a distributed mode. We\
    \ will use HTTP to interact with it. To check the available connector plugins,\
    \ you can run the following command:\n\n```\noc exec -i `oc get pods --field-selector\
    \ status.phase=Running -l strimzi.io/name=my-connect-cluster-connect -o=jsonpath='{.items[0].metadata.name}'`\
    \ -- curl -s http://my-connect-cluster-connect-api:8083/connector-plugins\n```\n\
    \n\nYou will find the two connectors that we are have added in the image.\n\n\
    ```\n[\n   ........\n   {\n      \"class\":\"org.apache.camel.kafkaconnector.minio.CamelMinioSinkConnector\"\
    ,\n      \"type\":\"sink\",\n      \"version\":\"0.5.0\"\n   },\n   {\n      \"\
    class\":\"org.apache.camel.kafkaconnector.minio.CamelMinioSourceConnector\",\n\
    \      \"type\":\"source\",\n      \"version\":\"0.5.0\"\n   },\n   {\n      \"\
    class\":\"org.apache.camel.kafkaconnector.sftp.CamelSftpSinkConnector\",\n   \
    \   \"type\":\"sink\",\n      \"version\":\"0.6.0-SNAPSHOT\"\n   },\n   {\n  \
    \    \"class\":\"org.apache.camel.kafkaconnector.sftp.CamelSftpSourceConnector\"\
    ,\n      \"type\":\"source\",\n      \"version\":\"0.6.0-SNAPSHOT\"\n   }\n  \
    \ .........\n]\n```\n\n## Create Source Connector Instance\n\nNow we can create\
    \ an instance connector that reads files from the AWS S3 liked object store (Minio).\
    \ Go to the text editor on the right, under the folder /root/camel-kafka. Right\
    \ click on the directory and choose New -> File and name it `minio-connector.yaml`.\n\
    \nPaste the following code into the application.\n\n<pre class=\"file\" data-filename=\"\
    minio-connector.yaml\" data-target=\"replace\">\napiVersion: kafka.strimzi.io/v1alpha1\n\
    kind: KafkaConnector\nmetadata:\n  name: minio-source-connector\n  labels:\n \
    \   strimzi.io/cluster: my-connect-cluster\nspec:\n  class: org.apache.camel.kafkaconnector.minio.CamelMinioSourceConnector\n\
    \  tasksMax: 1\n  config:\n    key.converter: org.apache.kafka.connect.storage.StringConverter\n\
    \    value.converter: org.apache.kafka.connect.storage.StringConverter\n    topics:\
    \ demo-topic\n    camel.source.path.bucketName: camel-kafka\n    camel.source.endpoint.initialDelay:\
    \ 20000\n    camel.source.endpoint.endpoint: http://minio:9000\n    camel.component.minio.accessKey:\
    \ minio\n    camel.component.minio.secretKey: minio123\n    camel.component.minio.operation:\
    \ getObject\n</pre>\n\nThis points the connector to listen to the camel-kafka\
    \ in the Minio object store, when detected content from the Minio source, the\
    \ connector will capture it and place it in the Kafka topic _*demo-topic*_ and\
    \ ready to be subscribed by the consumers. Execute following command to create\
    \ the source connector.\n\n```\noc create -f camel-kafka/minio-connector.yaml\n\
    ```\n\nNow, let's go ahead and place a file in the object store. This is going\
    \ to copy the healthcare related files that we have prepared into the object store.\n\
    \n```\noc rsync ./file/ `oc get pod -l app=minio -o=jsonpath='{.items[0].metadata.name}'`:/data/camel-kafka\n\
    ```\n\nNote the name of the two files.\n\n```\nWARNING: cannot use rsync: rsync\
    \ not available in container\nsample01.json\n```\n\nYou will find the file in\
    \ the Kafka Topic _*demo-topic*_\n\n```\noc exec -i `oc get pods --field-selector\
    \ status.phase=Running -l strimzi.io/name=my-connect-cluster-connect -o=jsonpath='{.items[0].metadata.name}'`\
    \ -- bin/kafka-console-consumer.sh --topic demo-topic --from-beginning --bootstrap-server\
    \ my-cluster-kafka-bootstrap:9092\n```\n\n\nWith content starting like following:\n\
    \n```\nOpenJDK 64-Bit Server VM warning: If the number of processors is expected\
    \ to increase from one, then you should configure the number of parallel GC threads\
    \ appropriately using -XX:ParallelGCThreads=N\n{\n  \"resourceType\": \"Observation\"\
    ,\n  \"id\": \"ekg\",\n  \"text\": {\n    \"status\": \"generated\",\n    \"div\"\
    : \"<div xmlns=\\\"http://www.w3.org/1999/xhtml\\\"><p><b>Generated Narrative\
    \ with Details</b></p><p><b>id</b>: ekg</p><p><b>status</b>: final</p><p><b>category</b>:\
    \ Procedure <span>(Details : {http://terminology.hl7.org/CodeSystem/observation-category\
    \ code 'procedure' = 'Procedure', given as 'Procedure'})</span></p><p><b>code</b>:\
    \ MDC_ECG_ELEC_POTL <span>(Details : {urn:oid:2.16.840.1.113883.6.24 code '131328'\
    \ = '131328', given as 'MDC_ECG_ELEC_POTL'})</span></p><p><b>subject</b>: <a>P.\
    \ van de Heuvel</a></p><p><b>effective</b>: 19/02/2015 9:30:35 AM</p><p><b>performer</b>:\
    \ <a>A. Langeveld</a></p><p><b>device</b>: 12 lead EKG Device Metric</p><blockquote><p><b>component</b></p><p><b>code</b>:\
    \ MDC_ECG_ELEC_POTL_I <span>(Details : {urn:oid:2.16.840.1.113883.6.24 code '131329'\
    \ = '131329', given as 'MDC_ECG_ELEC_POTL_I'})</span></p><p><b>value</b>: Origin:\
    \ (system = '[not stated]' code null = 'null'), Period: 10, Factor: 1.612, Lower:\
    \ -3300, Upper: 3300, Dimensions: 1, Data: 2041 2043 2037 2047 2060 2062 2051\
    \ 2023 2014 2027 2034 2033 2040 2047 2047 2053 2058 2064 2059 2063 2061 2052 2053\
    \ 2038 1966 1885 1884 2009 2129 2166 2137 2102 2086 2077 2067 2067 2060 2059 2062\
    \ 2062 2060 2057 2045 2047 2057 2054 2042 2029 2027 2018 2007 1995 2001 2012 2024\
    \ 2039 2068 2092 2111 2125 2131 2148 2137 2138 2128 2128 2115 2099 2097 2096 2101\
    \ 2101 2091 2073 2076 2077 2084 2081 2088 2092 2070 2069 2074 2077 2075 2068 2064\
    \ 2060 2062 2074 2075 2074 2075 2063 2058 2058 2064 2064 2070 2074 2067 2060 2062\
    \ 2063 2061 2059 2048 2052 2049 2048 2051 2059 2059 2066 2077 2073</p></blockquote><blockquote><p><b>component</b></p><p><b>code</b>:\
    \ MDC_ECG_ELEC_POTL_II <span>(Details : {urn:oid:2.16.840.1.113883.6.24 code '131330'\
    \ = '131330', given as 'MDC_ECG_ELEC_POTL_II'})</span></p><p><b>value</b>: Origin:\
    \ (system = '[not stated]' code null = 'null'), Period: 10, Factor: 1.612, Lower:\
    \ -3300, Upper: 3300, Dimensions: 1, Data: 2041 2043 2037 2047 2060 2062 2051\
    \ 2023 2014 2027 2034 2033 2040 2047 2047 2053 2058 2064 2059 2063 2061 2052 2053\
    \ 2038 1966 1885 1884 2009 2129 2166 2137 2102 2086 2077 2067 2067 2060 2059 2062\
    \ 2062 2060 2057 2045 2047 2057 2054 2042 2029 2027 2018 2007 1995 2001 2012 2024\
    \ 2039 2068 2092 2111 2125 2131 2148 2137 2138 2128 2128 2115 2099 2097 2096 2101\
    \ 2101 2091 2073 2076 2077 2084 2081 2088 2092 2070 2069 2074 2077 2075 2068 2064\
    \ 2060 2062 2074 2075 2074 2075 2063 2058 2058 2064 2064 2070 2074 2067 2060 2062\
    \ 2063 2061 2059 2048 2052 2049 2048 2051 2059 2059 2066 2077 2073</p></blockquote><blockquote><p><b>component</b></p><p><b>code</b>:\
    \ MDC_ECG_ELEC_POTL_III <span>(Details : {urn:oid:2.16.840.1.113883.6.24 code\
    \ '131389' = '131389', given as 'MDC_ECG_ELEC_POTL_III'})</span></p><p><b>value</b>:\
    \ Origin: (system = '[not stated]' code null = 'null'), Period: 10, Factor: 1.612,\
    \ Lower: -3300, Upper: 3300, Dimensions: 1, Data: 2041 2043 2037 2047 2060 2062\
    \ 2051 2023 2014 2027 2034 2033 2040 2047 2047 2053 2058 2064 2059 2063 2061 2052\
    \ 2053 2038 1966 1885 1884 2009 2129 2166 2137 2102 2086 2077 2067 2067 2060 2059\
    \ 2062 2062 2060 2057 2045 2047 2057 2054 2042 2029 2027 2018 2007 1995 2001 2012\
    \ 2024 2039 2068 2092 2111 2125 2131 2148 2137 2138 2128 2128 2115 2099 2097 2096\
    \ 2101 2101 2091 2073 2076 2077 2084 2081 2088 2092 2070 2069 2074 2077 2075 2068\
    \ 2064 2060 2062 2074 2075 2074 2075 2063 2058 2058 2064 2064 2070 2074 2067 2060\
    \ 2062 2063 2061 2059 2048 2052 2049 2048 2051 2059 2059 2066 2077 2073</p></blockquote></div>\"\
    \n  },\n  \"status\": \"final\",\n  \"category\": [\n    {\n      \"coding\":\
    \ [\n        {\n   ............\n```\n"
  difficulty: basic
  slug: step3
  tabs:
  - hostname: crc-nonest-1
    title: CLI
    type: terminal
  - hostname: crc-nonest-1
    port: 30443
    title: OpenShift Web Console
    type: service
  - hostname: crc-nonest-1
    path: /root
    title: Visual Editor
    type: code
  timelimit: 300
  title: Step 3
  type: challenge
- assignment: "## Create Sink Connector Instance\nFinally, we need to read the file\
    \ from the Kafka Topic, rename it into something our system understand and place\
    \ it into the SFTP server.\nWe need an instance of Camel Kafka connector that\
    \ move files from the updated topic to sink -- SFTP server. Go to the text editor\
    \ on the right, under the folder /root/camel-kafka. Right click on the directory\
    \ and choose New -> File and name it `sftp-connector.yaml`.\n\nPaste the following\
    \ code into the application.\n\n<pre class=\"file\" data-filename=\"sftp-connector.yaml\"\
    \ data-target=\"replace\">\napiVersion: kafka.strimzi.io/v1alpha1\nkind: KafkaConnector\n\
    metadata:\n  name: sftp-source-connector\n  labels:\n    strimzi.io/cluster: my-connect-cluster\n\
    spec:\n  class: org.apache.camel.kafkaconnector.sftp.CamelSftpSinkConnector\n\
    \  tasksMax: 1\n  config:\n    key.converter: org.apache.kafka.connect.storage.StringConverter\n\
    \    value.converter: org.apache.kafka.connect.storage.StringConverter\n    topics:\
    \ demo-topic\n    camel.sink.path.host: ftpserver\n    camel.sink.path.port: 22\n\
    \    camel.sink.path.directoryName: upload\n    camel.sink.endpoint.fileName:\
    \ ${date:now:yyyyMMddhhmmss}.json\n    camel.sink.endpoint.username: foo\n   \
    \ camel.sink.endpoint.password: pass\n    camel.sink.endpoint.useUserKnownHostsFile:\
    \ false\n    camel.sink.endpoint.privateKeyFile: /opt/kafka/external-configuration/sftp-ssh-key/demo_rsa\n\
    </pre>\n\nThis points the connector to the Kafka topic _*demo-topic*_ and write\
    \ the incoming event data into the SFTP server under the upload folder as configured,\
    \ with the name of the file changed to the yyyyMMddhhmmss format. Also points\
    \ to the keyfile that we have configured earlier in the secrets.\n\nRun command\
    \ to create the sink connector.\n\n```\noc create -f camel-kafka/sftp-connector.yaml\n\
    ```\n\nCheck if the file appears in the SFTP server.\n\n```\noc exec -i `oc get\
    \ pod -l deploymentconfig=ftpserver -o=jsonpath='{.items[0].metadata.name}'` --\
    \ ls /home/foo/upload/\n```\n\nYou should be seeing the same file that was updloaded!\n\
    ```\noc exec -i `oc get pod -l deploymentconfig=ftpserver -o=jsonpath='{.items[0].metadata.name}'`\
    \ -- find /home/foo/upload/ -type f -exec cat {} \\;\n```\n\n\n```\nOpenJDK 64-Bit\
    \ Server VM warning: If the number of processors is expected to increase from\
    \ one, then you should configure the number of parallel GC threads appropriately\
    \ using -XX:ParallelGCThreads=N\n{\n  \"resourceType\": \"Observation\",\n  \"\
    id\": \"ekg\",\n  \"text\": {\n    \"status\": \"generated\",\n    \"div\": \"\
    <div xmlns=\\\"http://www.w3.org/1999/xhtml\\\"><p><b>Generated Narrative with\
    \ Details</b></p><p><b>id</b>: ekg</p><p><b>status</b>: final</p><p><b>category</b>:\
    \ Procedure <span>(Details : {http://terminology.hl7.org/CodeSystem/observation-category\
    \ code 'procedure' = 'Procedure', given as 'Procedure'})</span></p><p><b>code</b>:\
    \ MDC_ECG_ELEC_POTL <span>(Details : {urn:oid:2.16.840.1.113883.6.24 code '131328'\
    \ = '131328', given as 'MDC_ECG_ELEC_POTL'})</span></p><p><b>subject</b>: <a>P.\
    \ van de Heuvel</a></p><p><b>effective</b>: 19/02/2015 9:30:35 AM</p><p><b>performer</b>:\
    \ <a>A. Langeveld</a></p><p><b>device</b>: 12 lead EKG Device Metric</p><blockquote><p><b>component</b></p><p><b>code</b>:\
    \ MDC_ECG_ELEC_POTL_I <span>(Details : {urn:oid:2.16.840.1.113883.6.24 code '131329'\
    \ = '131329', given as 'MDC_ECG_ELEC_POTL_I'})</span></p><p><b>value</b>: Origin:\
    \ (system = '[not stated]' code null = 'null'), Period: 10, Factor: 1.612, Lower:\
    \ -3300, Upper: 3300, Dimensions: 1, Data: 2041 2043 2037 2047 2060 2062 2051\
    \ 2023 2014 2027 2034 2033 2040 2047 2047 2053 2058 2064 2059 2063 2061 2052 2053\
    \ 2038 1966 1885 1884 2009 2129 2166 2137 2102 2086 2077 2067 2067 2060 2059 2062\
    \ 2062 2060 2057 2045 2047 2057 2054 2042 2029 2027 2018 2007 1995 2001 2012 2024\
    \ 2039 2068 2092 2111 2125 2131 2148 2137 2138 2128 2128 2115 2099 2097 2096 2101\
    \ 2101 2091 2073 2076 2077 2084 2081 2088 2092 2070 2069 2074 2077 2075 2068 2064\
    \ 2060 2062 2074 2075 2074 2075 2063 2058 2058 2064 2064 2070 2074 2067 2060 2062\
    \ 2063 2061 2059 2048 2052 2049 2048 2051 2059 2059 2066 2077 2073</p></blockquote><blockquote><p><b>component</b></p><p><b>code</b>:\
    \ MDC_ECG_ELEC_POTL_II <span>(Details : {urn:oid:2.16.840.1.113883.6.24 code '131330'\
    \ = '131330', given as 'MDC_ECG_ELEC_POTL_II'})</span></p><p><b>value</b>: Origin:\
    \ (system = '[not stated]' code null = 'null'), Period: 10, Factor: 1.612, Lower:\
    \ -3300, Upper: 3300, Dimensions: 1, Data: 2041 2043 2037 2047 2060 2062 2051\
    \ 2023 2014 2027 2034 2033 2040 2047 2047 2053 2058 2064 2059 2063 2061 2052 2053\
    \ 2038 1966 1885 1884 2009 2129 2166 2137 2102 2086 2077 2067 2067 2060 2059 2062\
    \ 2062 2060 2057 2045 2047 2057 2054 2042 2029 2027 2018 2007 1995 2001 2012 2024\
    \ 2039 2068 2092 2111 2125 2131 2148 2137 2138 2128 2128 2115 2099 2097 2096 2101\
    \ 2101 2091 2073 2076 2077 2084 2081 2088 2092 2070 2069 2074 2077 2075 2068 2064\
    \ 2060 2062 2074 2075 2074 2075 2063 2058 2058 2064 2064 2070 2074 2067 2060 2062\
    \ 2063 2061 2059 2048 2052 2049 2048 2051 2059 2059 2066 2077 2073</p></blockquote><blockquote><p><b>component</b></p><p><b>code</b>:\
    \ MDC_ECG_ELEC_POTL_III <span>(Details : {urn:oid:2.16.840.1.113883.6.24 code\
    \ '131389' = '131389', given as 'MDC_ECG_ELEC_POTL_III'})</span></p><p><b>value</b>:\
    \ Origin: (system = '[not stated]' code null = 'null'), Period: 10, Factor: 1.612,\
    \ Lower: -3300, Upper: 3300, Dimensions: 1, Data: 2041 2043 2037 2047 2060 2062\
    \ 2051 2023 2014 2027 2034 2033 2040 2047 2047 2053 2058 2064 2059 2063 2061 2052\
    \ 2053 2038 1966 1885 1884 2009 2129 2166 2137 2102 2086 2077 2067 2067 2060 2059\
    \ 2062 2062 2060 2057 2045 2047 2057 2054 2042 2029 2027 2018 2007 1995 2001 2012\
    \ 2024 2039 2068 2092 2111 2125 2131 2148 2137 2138 2128 2128 2115 2099 2097 2096\
    \ 2101 2101 2091 2073 2076 2077 2084 2081 2088 2092 2070 2069 2074 2077 2075 2068\
    \ 2064 2060 2062 2074 2075 2074 2075 2063 2058 2058 2064 2064 2070 2074 2067 2060\
    \ 2062 2063 2061 2059 2048 2052 2049 2048 2051 2059 2059 2066 2077 2073</p></blockquote></div>\"\
    \n  },\n  \"status\": \"final\",\n  \"category\": [\n    {\n      \"coding\":\
    \ [\n        {\n   ............\n```\n\n![overview](/openshift/assets/middleware/middleware-camelk/camel-kafka-connector/camel-kafka-step01-overview.png)\n\
    \n## Congratulations\n\nIn this scenario you got to play with Camel Kafka Connector.\
    \ You have initiated an AMQ Streams (Kafka) cluster on openshift, created an object\
    \ store, sftp server. Then started a Kafka Connect with your own build of image\
    \ including the libraries needed. Then you created a source Camel Kafka connector\
    \ that loads files from the _AWS2 S3_ like storage, place the file in the kafka\
    \ topic, and created another Camel Kafka connector that listens to the input from\
    \ the kafka topic and place the file in the SFTP server.\n"
  difficulty: basic
  slug: step4
  tabs:
  - hostname: crc-nonest-1
    title: CLI
    type: terminal
  - hostname: crc-nonest-1
    port: 30443
    title: OpenShift Web Console
    type: service
  - hostname: crc-nonest-1
    path: /root
    title: Visual Editor
    type: code
  timelimit: 300
  title: Step 4
  type: challenge
description: '

  This scenario will introduce [Camel Kafka Connector ](https://camel.apache.org/camel-kafka-connector/latest/index.html).


  ## What is Camel Kafka Connector?



  ![Logo](https://upload.wikimedia.org/wikipedia/commons/1/11/Apache_Camel_Logo.svg)



  ### Kafka Connector with a Camel twist -- connect to ALMOST anything


  Camel Kafka Connector enables you to use standard Camel components as Kafka Connect
  connectors. This widens the scope of possible integrations beyond the external systems
  supported by Kafka Connect connectors alone. Camel Kafka Connector works as an adapter
  that makes the popular Camel component ecosystem available in Kafka-based AMQ Streams
  on OpenShift.


  Camel Kafka Connector provides a user-friendly way to configure Camel components
  directly in the Kafka Connect framework. Using Camel Kafka Connector, you can leverage
  Camel components for integration with different systems by connecting to or from
  Camel Kafka sink or source connectors. You do not need to write any code, and can
  include the appropriate connector JARs in your Kafka Connect image and configure
  connector options using custom resources.



  ### AMQ Streams - Kubernetes-native Apache Kafka


  The Red Hat AMQ streams component is a massively scalable, distributed, and high-performance
  data streaming platform based on the Apache Kafka project. It offers a distributed
  backbone that allows microservices and other applications to share data with high
  throughput and low latency.


  As more applications move to Kubernetes and Red Hat OpenShift , it is increasingly
  important to be able to run the communication infrastructure on the same platform.
  Red Hat OpenShift, as a highly scalable platform, is a natural fit for messaging
  technologies such as Kafka. The AMQ streams component makes running and managing
  Apache Kafka OpenShift native through the use of powerful operators that simplify
  the deployment, configuration, management, and use of Apache Kafka on Red Hat OpenShift.

  '
developers:
- btannous@redhat.com
- nvinto@redhat.com
- rjarvine@redhat.com
icon: https://logodix.com/logo/1910931.png
level: beginner
owner: openshift
private: true
published: false
skipping_enabled: false
slug: serverless-camel-kafka-connector
tags:
- openshift
title: Camel Kafka Connector Basics
type: track
